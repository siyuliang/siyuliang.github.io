---
---

@string{acl = {Association for Computational Linguistics}}

@inproceedings{liang2025emnlp,
  title={Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels},
  author={Liang, Siyu and Ballier, Nicolas and Levow, Gina-Anne and Wright, Richard},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  year={2025},
  address={Suzhou, China},
  publisher=acl,
  selected={true},
  note={(to appear)},
  abbr={EMNLP}, 
  abstract={While large multilingual automatic speech recognition (ASR) models achieve remarkable performance, the internal mechanisms of the end-to-end pipeline, particularly concerning fairness and efficacy across languages, remain underexplored. This paper introduces a fine-grained analysis of Whisper’s multilingual decoder, examining its sub-token hypotheses during transcription across languages with various resource levels. Our method traces the beam search path, capturing sub-token guesses and their associated probabilities. Results reveal that higher resource languages benefit from higher likelihood of the correct token being top-ranked, greater confidence, lower predictive entropy, and more diverse alternative candidates. Lower resource languages fare worse on these metrics, but also exhibit distinct clustering patterns in sub-token usage sometimes influenced by typology in our PCA and t-SNE analysis. This sub-token probing uncovers systematic decoding disparities masked by aggregate error rates and points towards targeted interventions to ameliorate the imbalanced development of speech technology.}, 
  pdf={https://arxiv.org/abs/2509.25516}
}

@inproceedings{liang2025fieldmatters,
  title={Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages},
  author={Liang, Siyu and Levow, Gina-Anne},
  booktitle={Proceedings of the 4th Workshop on NLP Applications to Field Linguistics (FieldMatters)},
  year={2025},
  address={Vienna, Austria},
  publisher=acl,
  selected={true},
  abbr={ACL-FieldMatters}, 
  abstract={The development of Automatic Speech Recognition (ASR) has yielded impressive results, but its use in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation.}, 
  pdf={https://aclanthology.org/2025.fieldmatters-1.3/}
}

@inproceedings{liang2025sigtyp,
  title={Tone in Perspective: A Computational Typological Analysis of Tone Function in ASR},
  author={Liang, Siyu and Levow, Gina-Anne},
  booktitle={Proceedings of the 7th Workshop on Research in Computational Linguistic Typology and Multilingual NLP (SIGTYP)},
  year={2025},
  address={Vienna, Austria},
  publisher=acl,
  selected={false},
  abbr={ACL-SIGTYP},
  abstract={This study investigates the impact of pitch flattening on automatic speech recognition (ASR) performance across tonal and non-tonal languages. Using vocoder-based signal processing techniques, we created pitch-flattened versions of speech recordings and compared ASR performance against original recordings. Results reveal that tonal languages experience substantially larger performance degradation than non-tonal languages. Analysis of tone confusion matrices shows systematic patterns of misidentification where contour tones collapse toward level tones when pitch information is removed. Calculation of tone’s functional load at syllable and word levels demonstrates that syllable-level functional load strongly predicts ASR vulnerability to pitch flattening, while word-level patterns reflect each language’s morphological structure. These findings illuminate the differential importance of pitch information across languages and suggest that ASR systems for languages with high syllable-level functional load require more robust pitch modeling.}, 
  pdf={https://aclanthology.org/2025.sigtyp-1.11/}
}

@inproceedings{liang2020plc,
  title={Documenting Eynu: A Case Study of Language Contact},
  author={Liang, Siyu},
  booktitle={Proceedings of the 43rd Annual Penn Linguistics Conference},
  year={2020},
  selected={false},
  abbr={PLC},
  pdf={https://repository.upenn.edu/entities/publication/9582487d-e614-4e84-b0b6-bbdf57b1dbd9}
}